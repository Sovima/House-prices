{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading functions, data, and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be using code from previous version and collecting it into pipelines to simplify the data preprocessing process. On top of that, I will be using XGBRegressor to create a more accurate model. To note: the main difference between RandomForest and XGBRegressor is that while the first one collects results at the end, the latter is doing it throughout the modelling process, which allows it to identify the importance of certain branches over others. Then, pipeline allows for more efficient and consistent transformations because test set is transformed based on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the same as house prices file but with more mods\n",
    "#This is the same as house prices file but with more mods\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports are similar to the ones in version 2 however, additional imports such as Pipeline and XGBRegressor were added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains data importing process and functions that might be used for further analysis. The technology used in this cell is the same as the previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the data-------------------------------------------------------------------------------\n",
    "'''\n",
    "url_train = 'https://raw.githubusercontent.com/Sovima/House-prices/main/train.csv'" "\n",
    "url_test = \"https://raw.githubusercontent.com/Sovima/House-prices/main/test.csv\"" "\n",
    "house_data = pd.read_csv(url_train, error_bad_lines=False)\n",
    "test_data = pd.read_csv(url_test, error_bad_lines=False)\n",
    "y_train = house_data[\"SalePrice\"].copy()\n",
    "x_train = house_data.drop(\"SalePrice\", axis = 1).copy()\n",
    "\n",
    "'''\n",
    "Some useful functions----------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "def missing_mean(missing_mean_values, columns):\n",
    "    mean = missing_mean_values[columns].mean()\n",
    "    std = missing_mean_values[columns].std()\n",
    "    is_null = missing_mean_values[columns].isnull().sum()\n",
    "    rand_mean = np.random.randint(mean-std, mean+std, size = is_null)\n",
    "    house_dummies_slice = missing_mean_values[columns].copy()\n",
    "    house_dummies_slice[np.isnan(house_dummies_slice)] = rand_mean\n",
    "    missing_mean_values[columns] = house_dummies_slice\n",
    "    missing_mean_values[columns] = missing_mean_values[columns].astype(float)\n",
    "    \n",
    "def export_as_csv(predictions):\n",
    "    import os\n",
    "    os.chdir(\"Documents/Sofya/data s. corona/house-prices-advanced-regression-techniques\")\n",
    "    predictions.to_csv(\"Predictions_house\", index = False)\n",
    "\n",
    "def forest_model(house_dummies, SalePrice, test_dummies):\n",
    "    X_train = house_dummies\n",
    "    X_val = test_dummies\n",
    "    Y_train =  SalePrice\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_estimators=100)\n",
    "    random_forest.fit(X_train, Y_train)\n",
    "    Y_prediction = pd.DataFrame(random_forest.predict(X_val))\n",
    "    return Y_prediction\n",
    "\n",
    "def models(X, y):\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, \n",
    "                                                      y, \n",
    "                                                      random_state = 0)\n",
    "    #SGD Classifier\n",
    "    sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    Y_pred = sgd.predict(X_val)\n",
    "    acc_sgd = mean_absolute_error(Y_val, Y_pred)\n",
    "    #Random Forest\n",
    "    random_forest = RandomForestClassifier(n_estimators=100)\n",
    "    random_forest.fit(X_train, Y_train)\n",
    "    Y_pred = random_forest.predict(X_val)\n",
    "    acc_random_forest = mean_absolute_error(Y_val, Y_pred)\n",
    "    #Decision Tree\n",
    "    d_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "    d_tree.fit(X_train, Y_train)\n",
    "    Y_pred = d_tree.predict(X_val)\n",
    "    acc_decision_tree = mean_absolute_error(Y_val, Y_pred)\n",
    "    \n",
    "    #Regression\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    regr.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = regr.predict(X_val)\n",
    "    acc_linear_reg = mean_absolute_error(Y_val, Y_pred)\n",
    "    \n",
    "    return acc_sgd, acc_random_forest, acc_linear_reg, acc_decision_tree\n",
    "\n",
    "\n",
    "def missing_values(house_data):\n",
    "    total = house_data.isnull().sum().sort_values(ascending=False)\n",
    "    percent_1 = house_data.isnull().sum()/house_data.isnull().count()*100\n",
    "    percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "    return missing_data.head((missing_data['Total'] > 0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to define the transformations before applying them in the Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Imputer---------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "#GarageYrBlt was found to be an unnecessary column since it is similar to the year the house was built\n",
    "drop_columns = ['GarageYrBlt', 'SalePrice']\n",
    "\n",
    "num_columns = (set((house_data.dtypes == int).index[house_data.dtypes == int]) | \n",
    "               set((house_data.dtypes == float).index[house_data.dtypes == float])) - set(drop_columns)\n",
    "num_transformer = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Label Encoder-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "label_encoder_cols = [\n",
    "    'LotShape', \"Street\", \"MasVnrType\", 'LandContour', 'Utilities',\n",
    "    'LandSlope', 'BldgType', 'HouseStyle', 'ExterQual', 'ExterCond',\n",
    "    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "    'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
    "    'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "    'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature'\n",
    "    ]\n",
    "electrical_imputer = SimpleImputer(strategy = \"most_frequent\")\n",
    "label_imputer = SimpleImputer(strategy = \"constant\", fill_value = \"none\")\n",
    "\n",
    "good_encoder_cols = [col for col in label_encoder_cols if \n",
    "                   set(house_data[col]) >= set(test_data[col])]\n",
    "\n",
    "\"\"\"\n",
    "One Hot Encoder---------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "onehot_transformer =  Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "get_dummies_cols = set(label_encoder_cols)-set(good_encoder_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will use a pipeline to connect my transformers and transform the test and training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Connecting the Data----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "data_transformer = ColumnTransformer(transformers = [\n",
    "    ('number_transformer', num_transformer, list(num_columns)),\n",
    "    ('electrical_imputer', electrical_imputer, list([\"Electrical\"])),\n",
    "    ('label_imputer', label_imputer, list(good_encoder_cols)),\n",
    "    ('OneHot', onehot_transformer, list(get_dummies_cols))\n",
    "    ])\n",
    "\n",
    "concat_house = pd.DataFrame(data_transformer.fit_transform(x_train))\n",
    "concat_test = pd.DataFrame(data_transformer.transform(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have noticed, I did not include LabelEncoder and RandomForest into my pipeline. LabelEncoder does not work with pipelines so a custom transformation would have to be created(I will change this as I learn). Since model creation goes after data transformation, I cannot add Random Forest to the pipeline without doing LabelEncoder first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the part of the data that needs to be converted to a custom transformer\n",
    "my_encoder = LabelEncoder()\n",
    "for i in list(range(36, 63)):\n",
    "    concat_house[i] = (my_encoder.fit_transform(concat_house[i])).astype(float)\n",
    "    concat_test[i] = (my_encoder.transform(concat_test[i])).astype(float)\n",
    "\n",
    "concat_house = concat_house.astype(float)\n",
    "concat_test = concat_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is connected and transform I will eliminate columns with insignificant correlation. This will ensure no unnecessary variables are creating noise in the modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Finding columns with acceptable correlation-----------------------------------------------------------------\n",
    "'''\n",
    "corr = pd.DataFrame(concat_house.corrwith(y_train))\n",
    "#Following is an important function that allows to select df columns based on a condition\n",
    "#Should be memorized!\n",
    "\n",
    "high_corr_cols = set(corr.index[corr[0] > 0.2]) | set(corr.index[corr[0] < -0.2])\n",
    "\n",
    "#Note that sets can be used to select columns of a dataframe\n",
    "high_corr_house = concat_house[high_corr_cols]\n",
    "high_corr_test = concat_test[high_corr_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of the process is to create a model that will use data from the training set to predict the unknown house prices based on test_data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:56:12] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators = 1000, learning_rate = 0.2)\n",
    "my_model.fit(high_corr_house, y_train)\n",
    "predictions = (pd.DataFrame(test_data[\"Id\"].copy()))\n",
    "\n",
    "predictions[\"SalePrice\"] = (pd.DataFrame(my_model.predict(high_corr_test))).astype(float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
